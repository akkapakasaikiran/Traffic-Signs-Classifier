{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pickle \nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom math import sqrt, ceil\nfrom keras.utils.np_utils import to_categorical\n\nimport tensorflow as tf\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dropout, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, PReLU, Flatten, Dense, Input\nfrom keras.layers import Layer, concatenate\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.models import Model\nfrom keras.utils import plot_model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n\"\"\"\nDataset used is a preprocessed version of the German Traffic Signs Recognition Benchmark (GTSRB) dataset, details about which\ncan be found at [2]. This dataset is picked from [1]. The following code for reading data is borrowed from [3].\nIn particular data2.pickle is used, which performs shuffling and incorporates division by 255.0 followed by Mean Normalisation.\n \n\nReferences\n    [1] https://www.kaggle.com/valentynsichkar/traffic-signs-preprocessed\n    [2] https://www.kaggle.com/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign\n    [3] https://www.kaggle.com/valentynsichkar/traffic-signs-classification-with-cnn\n\"\"\"\n\n# Opening file for reading in binary mode\nwith open('../input/traffic-signs-preprocessed/data2.pickle', 'rb') as f:\n    data = pickle.load(f, encoding='latin1')  # dictionary type\n\n# Preparing y_train and y_validation for using in Keras\ndata['y_train'] = to_categorical(data['y_train'], num_classes=43)\ndata['y_validation'] = to_categorical(data['y_validation'], num_classes=43)\n\n# Making channels come at the end\ndata['x_train'] = data['x_train'].transpose(0, 2, 3, 1)\ndata['x_validation'] = data['x_validation'].transpose(0, 2, 3, 1)\ndata['x_test'] = data['x_test'].transpose(0, 2, 3, 1)\n\n# Showing loaded data from file\nfor i, j in data.items():\n    if i == 'labels':\n        print(i + ':', len(j))\n    else: \n        print(i + ':', j.shape)","execution_count":12,"outputs":[{"output_type":"stream","text":"y_test: (12630,)\ny_validation: (4410, 43)\nx_validation: (4410, 32, 32, 3)\nx_train: (86989, 32, 32, 3)\ny_train: (86989, 43)\nlabels: 43\nx_test: (12630, 32, 32, 3)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"          \n\"\"\"\nInception Modules\n\nAn implementation of Google's Inception modules, introduced in GoogLeNet ([1]), done from scrath using Layer Subclassing.\nThe design of these implementations differs a bit from that of the original, and the deviations are inspired by [2].\n\nReferences\n    [1] Going deeper with convolutions, Christian Szegedy, Wei Liu, 2015\n    [2] Traffic Sign Classification Using Deep Inception Based Convolutional Networks, Mrinal Hanoi, 2015\n\n\"\"\"\nclass InceptionA(Layer):\n    def __init__(self, ch_out_1, ch_out_3, ch_out_5, ch_out_m):\n        super(InceptionA, self).__init__()\n        self.conv1_1 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        \n        self.conv1_2 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_1 = Conv2D(ch_out_3, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_3 = Conv2D(ch_out_1, 1, activation=\"relu\", padding=\"same\")\n        self.conv5_1 = Conv2D(ch_out_5, 5, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_4 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_2 = Conv2D(ch_out_m, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.mp = MaxPool2D(pool_size=3, padding=\"same\", strides=1)\n        \n        \n    def call(self, inputs):\n        x = self.conv1_1(inputs)\n        \n        y = self.conv1_2(inputs)\n        y = self.conv3_1(y)\n        \n        z = self.conv1_3(inputs)\n        z = self.conv5_1(z)\n        \n        w = self.conv1_4(inputs)\n        w = self.conv3_2(w)\n        w = self.mp(w)\n        \n        outputs = concatenate([x,y,z,w], axis=3)\n        \n        return outputs\n    \n    \nclass Inception4A(Layer):\n    def __init__(self, ch_out_1, ch_out_3, ch_out_5, ch_out_m):\n        super(Inception4A, self).__init__()\n        self.conv1_1 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        \n        self.conv1_2 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_1 = Conv2D(ch_out_3, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_3 = Conv2D(ch_out_1, 1, activation=\"relu\", padding=\"same\")\n        self.conv3_2 = Conv2D(ch_out_5, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv3_3 = Conv2D(ch_out_5, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_4 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_4 = Conv2D(ch_out_m, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.mp = MaxPool2D(pool_size=3, padding=\"same\", strides=1)\n        \n        \n    def call(self, inputs):\n        x = self.conv1_1(inputs)\n        \n        y = self.conv1_2(inputs)\n        y = self.conv3_1(y)\n        \n        z = self.conv1_3(inputs)\n        z = self.conv3_2(z)\n        z = self.conv3_3(z)\n        \n        w = self.conv1_4(inputs)\n        w = self.conv3_4(w)\n        w = self.mp(w)\n        \n        outputs = concatenate([x,y,z,w], axis=3)\n        \n        return outputs\n    \nclass Inception4B(Layer):\n    def __init__(self, ch_out_1, ch_out_3, ch_out_5, ch_out_m):\n        super(Inception4B, self).__init__()\n        self.conv1_1 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        \n        self.conv1_2 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv7_1 = Conv2D(ch_out_3, (7,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv7_2 = Conv2D(ch_out_3, (1,7), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_3 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv7_3 = Conv2D(ch_out_5, (7,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv7_4 = Conv2D(ch_out_5, (1,7), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv7_5 = Conv2D(ch_out_5, (7,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv7_6 = Conv2D(ch_out_5, (1,7), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_4 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_1 = Conv2D(ch_out_m, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.mp = MaxPool2D(pool_size=3, padding=\"same\", strides=1)\n        \n        \n    def call(self, inputs):\n        x = self.conv1_1(inputs)\n        \n        y = self.conv1_2(inputs)\n        y = self.conv7_1(y)\n        y = self.conv7_2(y)\n        \n        z = self.conv1_3(inputs)\n        z = self.conv7_3(z)\n        z = self.conv7_4(z)\n        z = self.conv7_5(z)\n        z = self.conv7_6(z)\n        \n        w = self.conv1_4(inputs)\n        w = self.conv3_1(w)\n        w = self.mp(w)\n        \n        outputs = concatenate([x,y,z,w], axis=3)\n        \n        return outputs\n    \nclass Inception4C(Layer):\n    def __init__(self, ch_out_1, ch_out_3, ch_out_5, ch_out_m):\n        super(Inception4C, self).__init__()\n        self.conv1_1 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        \n        self.conv1_2 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_1 = Conv2D(ch_out_3/2, (1,3), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv3_2 = Conv2D(ch_out_3/2, (3,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_3 = Conv2D(ch_out_1, 1, activation=\"relu\", padding=\"same\")\n        self.conv3_3 = Conv2D(ch_out_5, (1,3), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv3_4 = Conv2D(ch_out_5, (3,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv3_5 = Conv2D(ch_out_5/2, (1,3), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv3_6 = Conv2D(ch_out_5/2, (3,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_4 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_7 = Conv2D(ch_out_m, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.mp = MaxPool2D(pool_size=3, padding=\"same\", strides=1)\n        \n        \n    def call(self, inputs):\n        x = self.conv1_1(inputs)\n        \n        y = self.conv1_2(inputs)\n        y_1 = self.conv3_1(y)\n        y_2 = self.conv3_2(y)\n        y = concatenate([y_1, y_2], axis=3)\n        \n        z = self.conv1_3(inputs)\n        z = self.conv3_3(z)\n        z = self.conv3_4(z)\n        z_1 = self.conv3_5(z)\n        z_2 = self.conv3_6(z)\n        z = concatenate([z_1, z_2], axis=3)\n        \n        w = self.conv1_4(inputs)\n        w = self.conv3_7(w)\n        w = self.mp(w)\n        \n        outputs = concatenate([x, y, z, w], axis=3)\n        \n        return outputs","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nSpatial Transformer Network    \n    \nIntroduced in [1], these networks learn an affine transformation to get the interesting part of the image into focus.\nThis implementation is borrowed from [2], and a lot was learnt prior to that from [3].\n    \nHere's an example usage of STN()\n\nm = Sequential()\nm.add(STN(input_shape=(16,16,3), filter_size=3))\nm.add(Conv2D(64, 3, padding='same'))\nm.add(MaxPool2D())\nm.add(Conv2D(64, 3, padding='same'))\nm.add(Dropout(0.5))\nm.add(Flatten())\nm.add(Dense(43, activation='softmax'))\n\nm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nm.summary()\n\nReferences\n    ----------\n    [1]  Spatial Transformer Networks, Max Jaderberg, et al., 2015\n    [2]  https://github.com/mbanf/STN.keras\n    [3]  https://github.com/hello2all/GTSRB_Keras_STN\n\"\"\"\n\ndef K_meshgrid(x, y):\n    return tf.meshgrid(x, y)\n\ndef K_linspace(start, stop, num):\n    return tf.linspace(start, stop, num)\n\nclass BilinearInterpolation(Layer):\n\n    def __init__(self, output_size, **kwargs):\n        self.output_size = output_size\n        super(BilinearInterpolation, self).__init__(**kwargs)\n\n    def call(self, tensors, mask=None):\n        X, transformation = tensors\n        output = self._transform(X, transformation, self.output_size)\n        return output\n\n    def _interpolate(self, image, sampled_grids, output_size):\n\n        batch_size = K.shape(image)[0]\n        height = K.shape(image)[1]\n        width = K.shape(image)[2]\n        num_channels = K.shape(image)[3]\n\n        x = K.cast(K.flatten(sampled_grids[:, 0:1, :]), dtype='float32')\n        y = K.cast(K.flatten(sampled_grids[:, 1:2, :]), dtype='float32')\n\n        x = .5 * (x + 1.0) * K.cast(width, dtype='float32')\n        y = .5 * (y + 1.0) * K.cast(height, dtype='float32')\n\n        x0 = K.cast(x, 'int32')\n        x1 = x0 + 1\n        y0 = K.cast(y, 'int32')\n        y1 = y0 + 1\n\n        max_x = int(K.int_shape(image)[2] - 1)\n        max_y = int(K.int_shape(image)[1] - 1)\n\n        x0 = K.clip(x0, 0, max_x)\n        x1 = K.clip(x1, 0, max_x)\n        y0 = K.clip(y0, 0, max_y)\n        y1 = K.clip(y1, 0, max_y)\n\n        pixels_batch = K.arange(0, batch_size) * (height * width)\n        pixels_batch = K.expand_dims(pixels_batch, axis=-1)\n        flat_output_size = output_size[0] * output_size[1]\n        base = K.repeat_elements(pixels_batch, flat_output_size, axis=1)\n        base = K.flatten(base)\n\n        base_y0 = y0 * width\n        base_y0 = base + base_y0\n        base_y1 = y1 * width\n        base_y1 = base_y1 + base\n\n        indices_a = base_y0 + x0\n        indices_b = base_y1 + x0\n        indices_c = base_y0 + x1\n        indices_d = base_y1 + x1\n\n        flat_image = K.reshape(image, shape=(-1, num_channels))\n        flat_image = K.cast(flat_image, dtype='float32')\n        pixel_values_a = K.gather(flat_image, indices_a)\n        pixel_values_b = K.gather(flat_image, indices_b)\n        pixel_values_c = K.gather(flat_image, indices_c)\n        pixel_values_d = K.gather(flat_image, indices_d)\n\n        x0 = K.cast(x0, 'float32')\n        x1 = K.cast(x1, 'float32')\n        y0 = K.cast(y0, 'float32')\n        y1 = K.cast(y1, 'float32')\n\n        area_a = K.expand_dims(((x1 - x) * (y1 - y)), 1)\n        area_b = K.expand_dims(((x1 - x) * (y - y0)), 1)\n        area_c = K.expand_dims(((x - x0) * (y1 - y)), 1)\n        area_d = K.expand_dims(((x - x0) * (y - y0)), 1)\n\n        values_a = area_a * pixel_values_a\n        values_b = area_b * pixel_values_b\n        values_c = area_c * pixel_values_c\n        values_d = area_d * pixel_values_d\n        return values_a + values_b + values_c + values_d\n\n    def _make_regular_grids(self, batch_size, height, width):\n        # making a single regular grid\n        x_linspace = K_linspace(-1., 1., width)\n        y_linspace = K_linspace(-1., 1., height)\n        x_coordinates, y_coordinates = K_meshgrid(x_linspace, y_linspace)\n        x_coordinates = K.flatten(x_coordinates)\n        y_coordinates = K.flatten(y_coordinates)\n        ones = K.ones_like(x_coordinates)\n        grid = K.concatenate([x_coordinates, y_coordinates, ones], 0)\n\n        # repeating grids for each batch\n        grid = K.flatten(grid)\n        grids = K.tile(grid, K.stack([batch_size]))\n        return K.reshape(grids, (batch_size, 3, height * width))\n\n    def _transform(self, X, affine_transformation, output_size):\n        batch_size = K.shape(X)[0]\n        transformations = K.reshape(affine_transformation, shape=(batch_size, 2, 3))\n        regular_grids = self._make_regular_grids(batch_size, output_size[0], output_size[1])\n        sampled_grids = K.batch_dot(transformations, regular_grids)\n        interpolated_image = self._interpolate(X, sampled_grids, output_size)\n        new_shape = (batch_size, output_size[0], output_size[1], output_size[2])\n        interpolated_image = K.reshape(interpolated_image, new_shape)\n        return interpolated_image\n    \ndef get_initial_weights(output_size):\n    b = np.zeros((2, 3), dtype='float32')\n    b[0, 0] = 1\n    b[1, 1] = 1\n    W = np.zeros((output_size, 6), dtype='float32')\n    weights = [W, b.flatten()]\n    return weights\n\n\ndef STN(input_shape=(32, 32, 3), filter_size=5):\n    image = Input(shape=input_shape)\n    locnet = Conv2D(64, 5, strides=2, activation='relu')(image)\n    locnet = MaxPool2D()(locnet)\n    locnet = Conv2D(128, filter_size, activation='relu')(locnet)\n    locnet = Flatten()(locnet)\n    locnet = Dense(128)(locnet)\n    weights = get_initial_weights(128)\n    locnet = Dense(6, weights=weights)(locnet)\n    out = BilinearInterpolation(input_shape)([image, locnet])\n    return Model(inputs=image, outputs=out)\n","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nA Deep Convolutional Neural Network\n\nModel inspired by [1], but uses more recent Inception Modules. \nThe presented network consists of spatial transformer layers and modified versions of inception modules specifically designed for capturing \nlocal and global features together. This features adoption allows the network to classify precisely intraclass samples even under deformations.\nUse of spatial transformer layer makes this network more robust to deformations such as translation, rotation, scaling of input images.\n\nReferences\n    ---------\n    [1] Traffic Sign Classification Using Deep Inception Based Convolutional Networks, Mrinal Hanoi, 2015\n\n\"\"\"\n\ninput_shape = (32,32,3)\n\nmodel = Sequential()\nmodel.add(STN(input_shape))\nmodel.add(Conv2D(64, 3, activation=PReLU(), padding='same'))\nmodel.add(STN((32,32,64)))\nmodel.add(Conv2D(64, 3, activation=PReLU(), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D())\n\nmodel.add(STN((16,16,64), 3))\nmodel.add(Inception4A(32, 64, 16, 16))\nmodel.add(STN((16,16,128), 3))\nmodel.add(Inception4A(32, 64, 16, 16))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D())\n\nmodel.add(Inception4B(64, 128, 32, 32))\nmodel.add(Inception4B(64, 128, 32, 32))\nmodel.add(Inception4B(64, 128, 32, 32))\nmodel.add(Inception4B(64, 128, 32, 32))\nmodel.add(Inception4B(64, 128, 32, 32))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D())\n\nmodel.add(Inception4C(128, 256, 64, 64))\nmodel.add(Inception4C(128, 256, 64, 64))\nmodel.add(AvgPool2D(pool_size=4))\n\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(43, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()","execution_count":15,"outputs":[{"output_type":"stream","text":"Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nmodel_4 (Model)              (None, 32, 32, 3)         358150    \n_________________________________________________________________\nconv2d_107 (Conv2D)          (None, 32, 32, 64)        67328     \n_________________________________________________________________\nmodel_5 (Model)              (None, 32, 32, 64)        455750    \n_________________________________________________________________\nconv2d_110 (Conv2D)          (None, 32, 32, 64)        102464    \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 32, 32, 64)        256       \n_________________________________________________________________\nmax_pooling2d_19 (MaxPooling (None, 16, 16, 64)        0         \n_________________________________________________________________\nmodel_6 (Model)              (None, 16, 16, 64)        193606    \n_________________________________________________________________\ninception4a_2 (Inception4A)  (None, 16, 16, 128)       67056     \n_________________________________________________________________\nmodel_7 (Model)              (None, 16, 16, 128)       296006    \n_________________________________________________________________\ninception4a_3 (Inception4A)  (None, 16, 16, 128)       75248     \n_________________________________________________________________\nbatch_normalization_4 (Batch (None, 16, 16, 128)       512       \n_________________________________________________________________\nmax_pooling2d_24 (MaxPooling (None, 8, 8, 128)         0         \n_________________________________________________________________\ninception4b_5 (Inception4B)  (None, 8, 8, 256)         286368    \n_________________________________________________________________\ninception4b_6 (Inception4B)  (None, 8, 8, 256)         319136    \n_________________________________________________________________\ninception4b_7 (Inception4B)  (None, 8, 8, 256)         319136    \n_________________________________________________________________\ninception4b_8 (Inception4B)  (None, 8, 8, 256)         319136    \n_________________________________________________________________\ninception4b_9 (Inception4B)  (None, 8, 8, 256)         319136    \n_________________________________________________________________\nbatch_normalization_5 (Batch (None, 8, 8, 256)         1024      \n_________________________________________________________________\nmax_pooling2d_30 (MaxPooling (None, 4, 4, 256)         0         \n_________________________________________________________________\ninception4c_2 (Inception4C)  (None, 4, 4, 512)         361472    \n_________________________________________________________________\ninception4c_3 (Inception4C)  (None, 4, 4, 512)         492544    \n_________________________________________________________________\naverage_pooling2d_1 (Average (None, 1, 1, 512)         0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 1, 1, 512)         0         \n_________________________________________________________________\nflatten_10 (Flatten)         (None, 512)               0         \n_________________________________________________________________\ndense_19 (Dense)             (None, 43)                22059     \n=================================================================\nTotal params: 4,056,387\nTrainable params: 4,055,491\nNon-trainable params: 896\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nTraining and Testing\n\nAn annealer is used to control lerning rate, reducing it as epochs increase so that fine learning can take place. \nIdea attributed to [1].\n\nReferences \n    -----------\n    [1] https://www.kaggle.com/valentynsichkar/traffic-signs-classification-with-cnn\n\"\"\"\n\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs))\nepochs = 30\n\nh = model.fit(data['x_train'], data['y_train'],\n              batch_size=20, epochs=epochs,\n              validation_data=(data['x_validation'], data['y_validation']),\n              callbacks=[annealer], verbose=1)\n\n# Test Accuracy\npred = model.predict(data['x_test'])\ncorr = np.argmax(pred, axis=1)\nacc = np.mean(corr == data['y_test'])\nprint(\"Test accuracy: \", acc)","execution_count":16,"outputs":[{"output_type":"stream","text":"Epoch 1/30\n4350/4350 [==============================] - 232s 53ms/step - loss: 3.0390 - accuracy: 0.7101 - val_loss: 0.5295 - val_accuracy: 0.9324 - lr: 2.1464e-04\nEpoch 2/30\n4350/4350 [==============================] - 229s 53ms/step - loss: 0.3295 - accuracy: 0.9670 - val_loss: 0.2563 - val_accuracy: 0.9705 - lr: 2.0391e-04\nEpoch 3/30\n4350/4350 [==============================] - 228s 52ms/step - loss: 0.2107 - accuracy: 0.9788 - val_loss: 0.1742 - val_accuracy: 0.9828 - lr: 1.9371e-04\nEpoch 4/30\n4350/4350 [==============================] - 227s 52ms/step - loss: 0.1600 - accuracy: 0.9843 - val_loss: 0.1380 - val_accuracy: 0.9866 - lr: 1.8403e-04\nEpoch 5/30\n4350/4350 [==============================] - 228s 52ms/step - loss: 0.1324 - accuracy: 0.9871 - val_loss: 0.1693 - val_accuracy: 0.9762 - lr: 1.7482e-04\nEpoch 6/30\n4350/4350 [==============================] - 228s 52ms/step - loss: 0.1076 - accuracy: 0.9894 - val_loss: 0.1121 - val_accuracy: 0.9884 - lr: 1.6608e-04\nEpoch 7/30\n4350/4350 [==============================] - 222s 51ms/step - loss: 0.0937 - accuracy: 0.9912 - val_loss: 0.1210 - val_accuracy: 0.9755 - lr: 1.5778e-04\nEpoch 8/30\n4350/4350 [==============================] - 217s 50ms/step - loss: 0.0820 - accuracy: 0.9921 - val_loss: 0.1094 - val_accuracy: 0.9823 - lr: 1.4989e-04\nEpoch 9/30\n4350/4350 [==============================] - 219s 50ms/step - loss: 0.0727 - accuracy: 0.9927 - val_loss: 0.0666 - val_accuracy: 0.9927 - lr: 1.4240e-04\nEpoch 10/30\n4350/4350 [==============================] - 221s 51ms/step - loss: 0.0629 - accuracy: 0.9944 - val_loss: 0.0879 - val_accuracy: 0.9857 - lr: 1.3528e-04\nEpoch 11/30\n4350/4350 [==============================] - 219s 50ms/step - loss: 0.0577 - accuracy: 0.9945 - val_loss: 0.0843 - val_accuracy: 0.9866 - lr: 1.2851e-04\nEpoch 12/30\n4350/4350 [==============================] - 224s 51ms/step - loss: 0.0533 - accuracy: 0.9948 - val_loss: 0.0706 - val_accuracy: 0.9900 - lr: 1.2209e-04\nEpoch 13/30\n4350/4350 [==============================] - 224s 52ms/step - loss: 0.0450 - accuracy: 0.9960 - val_loss: 0.0766 - val_accuracy: 0.9871 - lr: 1.1598e-04\nEpoch 14/30\n4350/4350 [==============================] - 225s 52ms/step - loss: 0.0442 - accuracy: 0.9958 - val_loss: 0.0563 - val_accuracy: 0.9923 - lr: 1.1018e-04\nEpoch 15/30\n4350/4350 [==============================] - 228s 52ms/step - loss: 0.0384 - accuracy: 0.9965 - val_loss: 0.0641 - val_accuracy: 0.9880 - lr: 1.0467e-04\nEpoch 16/30\n4350/4350 [==============================] - 224s 52ms/step - loss: 0.0352 - accuracy: 0.9968 - val_loss: 0.0515 - val_accuracy: 0.9925 - lr: 9.9440e-05\nEpoch 17/30\n4350/4350 [==============================] - 225s 52ms/step - loss: 0.0325 - accuracy: 0.9971 - val_loss: 0.0587 - val_accuracy: 0.9887 - lr: 9.4468e-05\nEpoch 18/30\n4350/4350 [==============================] - 225s 52ms/step - loss: 0.0294 - accuracy: 0.9975 - val_loss: 0.0797 - val_accuracy: 0.9841 - lr: 8.9745e-05\nEpoch 19/30\n4350/4350 [==============================] - 224s 52ms/step - loss: 0.0295 - accuracy: 0.9972 - val_loss: 0.0494 - val_accuracy: 0.9884 - lr: 8.5258e-05\nEpoch 20/30\n4350/4350 [==============================] - 231s 53ms/step - loss: 0.0247 - accuracy: 0.9980 - val_loss: 0.0387 - val_accuracy: 0.9937 - lr: 8.0995e-05\nEpoch 21/30\n4350/4350 [==============================] - 231s 53ms/step - loss: 0.0244 - accuracy: 0.9980 - val_loss: 0.0306 - val_accuracy: 0.9959 - lr: 7.6945e-05\nEpoch 22/30\n4350/4350 [==============================] - 232s 53ms/step - loss: 0.0229 - accuracy: 0.9979 - val_loss: 0.0815 - val_accuracy: 0.9805 - lr: 7.3098e-05\nEpoch 23/30\n4350/4350 [==============================] - 227s 52ms/step - loss: 0.0198 - accuracy: 0.9984 - val_loss: 0.0875 - val_accuracy: 0.9830 - lr: 6.9443e-05\nEpoch 24/30\n4350/4350 [==============================] - 224s 52ms/step - loss: 0.0195 - accuracy: 0.9984 - val_loss: 0.0395 - val_accuracy: 0.9914 - lr: 6.5971e-05\nEpoch 25/30\n4350/4350 [==============================] - 226s 52ms/step - loss: 0.0190 - accuracy: 0.9983 - val_loss: 0.0276 - val_accuracy: 0.9966 - lr: 6.2672e-05\nEpoch 26/30\n4350/4350 [==============================] - 227s 52ms/step - loss: 0.0157 - accuracy: 0.9989 - val_loss: 0.0245 - val_accuracy: 0.9964 - lr: 5.9539e-05\nEpoch 27/30\n4350/4350 [==============================] - 230s 53ms/step - loss: 0.0141 - accuracy: 0.9990 - val_loss: 0.0445 - val_accuracy: 0.9927 - lr: 5.6562e-05\nEpoch 28/30\n4350/4350 [==============================] - 230s 53ms/step - loss: 0.0140 - accuracy: 0.9989 - val_loss: 0.0237 - val_accuracy: 0.9959 - lr: 5.3734e-05\nEpoch 29/30\n4350/4350 [==============================] - 228s 52ms/step - loss: 0.0130 - accuracy: 0.9990 - val_loss: 0.0649 - val_accuracy: 0.9873 - lr: 5.1047e-05\nEpoch 30/30\n4350/4350 [==============================] - 229s 53ms/step - loss: 0.0120 - accuracy: 0.9991 - val_loss: 0.0321 - val_accuracy: 0.9932 - lr: 4.8495e-05\nTest accuracy:  0.9752177355502771\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}

{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pickle \nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom math import sqrt, ceil\nfrom keras.utils.np_utils import to_categorical\n\nimport tensorflow as tf\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dropout, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, PReLU, Flatten, Dense, Input\nfrom keras.layers import Layer, concatenate\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.models import Model\nfrom keras.utils import plot_model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n\"\"\"\nDataset used can be found at [1].\nIn particular we use data2, which performs shuffling and incorporates /255.0 + Mean Normalisation\n\nReferences\n    [1] https://www.kaggle.com/valentynsichkar/traffic-signs-preprocessed\n\"\"\"\n\n# Opening file for reading in binary mode\nwith open('../input/traffic-signs-preprocessed/data2.pickle', 'rb') as f:\n    data = pickle.load(f, encoding='latin1')  # dictionary type\n\n# Preparing y_train and y_validation for using in Keras\ndata['y_train'] = to_categorical(data['y_train'], num_classes=43)\ndata['y_validation'] = to_categorical(data['y_validation'], num_classes=43)\n\n# Making channels come at the end\ndata['x_train'] = data['x_train'].transpose(0, 2, 3, 1)\ndata['x_validation'] = data['x_validation'].transpose(0, 2, 3, 1)\ndata['x_test'] = data['x_test'].transpose(0, 2, 3, 1)\n\n# Showing loaded data from file\n# for i, j in data.items():\n#     if i == 'labels':\n#         print(i + ':', len(j))\n#     else: \n#         print(i + ':', j.shape)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"          \n\"\"\"\nInception Modules\n\nAn implementation of Google's Inception modules, introduced in GoogLeNet ([1]), done from scrath using Layer Subclassing.\nThe design of these implementations differs a bit from that of the original,and the deviations are inspired by [2]\n\nReferences\n    [1] Going deeper with convolutions, Christian Szegedy, Wei Liu, 2015\n    [2] Traffic Sign Classification Using Deep Inception Based Convolutional Networks, Mrinal Hanoi, 2015\n\n\"\"\"\nclass InceptionA(Layer):\n    def __init__(self, ch_out_1, ch_out_3, ch_out_5, ch_out_m):\n        super(InceptionA, self).__init__()\n        self.conv1_1 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        \n        self.conv1_2 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_1 = Conv2D(ch_out_3, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_3 = Conv2D(ch_out_1, 1, activation=\"relu\", padding=\"same\")\n        self.conv5_1 = Conv2D(ch_out_5, 5, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_4 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_2 = Conv2D(ch_out_m, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.mp = MaxPool2D(pool_size=3, padding=\"same\", strides=1)\n        \n        \n    def call(self, inputs):\n        x = self.conv1_1(inputs)\n        \n        y = self.conv1_2(inputs)\n        y = self.conv3_1(y)\n        \n        z = self.conv1_3(inputs)\n        z = self.conv5_1(z)\n        \n        w = self.conv1_4(inputs)\n        w = self.conv3_2(w)\n        w = self.mp(w)\n        \n        outputs = concatenate([x,y,z,w], axis=3)\n        \n        return outputs\n    \n    \nclass Inception4A(Layer):\n    def __init__(self, ch_out_1, ch_out_3, ch_out_5, ch_out_m):\n        super(Inception4A, self).__init__()\n        self.conv1_1 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        \n        self.conv1_2 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_1 = Conv2D(ch_out_3, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_3 = Conv2D(ch_out_1, 1, activation=\"relu\", padding=\"same\")\n        self.conv3_2 = Conv2D(ch_out_5, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv3_3 = Conv2D(ch_out_5, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_4 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_4 = Conv2D(ch_out_m, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.mp = MaxPool2D(pool_size=3, padding=\"same\", strides=1)\n        \n        \n    def call(self, inputs):\n        x = self.conv1_1(inputs)\n        \n        y = self.conv1_2(inputs)\n        y = self.conv3_1(y)\n        \n        z = self.conv1_3(inputs)\n        z = self.conv3_2(z)\n        z = self.conv3_3(z)\n        \n        w = self.conv1_4(inputs)\n        w = self.conv3_4(w)\n        w = self.mp(w)\n        \n        outputs = concatenate([x,y,z,w], axis=3)\n        \n        return outputs\n    \nclass Inception4B(Layer):\n    def __init__(self, ch_out_1, ch_out_3, ch_out_5, ch_out_m):\n        super(Inception4B, self).__init__()\n        self.conv1_1 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        \n        self.conv1_2 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv7_1 = Conv2D(ch_out_3, (7,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv7_2 = Conv2D(ch_out_3, (1,7), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_3 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv7_3 = Conv2D(ch_out_5, (7,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv7_4 = Conv2D(ch_out_5, (1,7), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv7_5 = Conv2D(ch_out_5, (7,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv7_6 = Conv2D(ch_out_5, (1,7), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_4 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_1 = Conv2D(ch_out_m, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.mp = MaxPool2D(pool_size=3, padding=\"same\", strides=1)\n        \n        \n    def call(self, inputs):\n        x = self.conv1_1(inputs)\n        \n        y = self.conv1_2(inputs)\n        y = self.conv7_1(y)\n        y = self.conv7_2(y)\n        \n        z = self.conv1_3(inputs)\n        z = self.conv7_3(z)\n        z = self.conv7_4(z)\n        z = self.conv7_5(z)\n        z = self.conv7_6(z)\n        \n        w = self.conv1_4(inputs)\n        w = self.conv3_1(w)\n        w = self.mp(w)\n        \n        outputs = concatenate([x,y,z,w], axis=3)\n        \n        return outputs\n    \nclass Inception4C(Layer):\n    def __init__(self, ch_out_1, ch_out_3, ch_out_5, ch_out_m):\n        super(Inception4C, self).__init__()\n        self.conv1_1 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        \n        self.conv1_2 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_1 = Conv2D(ch_out_3/2, (1,3), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv3_2 = Conv2D(ch_out_3/2, (3,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_3 = Conv2D(ch_out_1, 1, activation=\"relu\", padding=\"same\")\n        self.conv3_3 = Conv2D(ch_out_5, (1,3), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv3_4 = Conv2D(ch_out_5, (3,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv3_5 = Conv2D(ch_out_5/2, (1,3), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv3_6 = Conv2D(ch_out_5/2, (3,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_4 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_7 = Conv2D(ch_out_m, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.mp = MaxPool2D(pool_size=3, padding=\"same\", strides=1)\n        \n        \n    def call(self, inputs):\n        x = self.conv1_1(inputs)\n        \n        y = self.conv1_2(inputs)\n        y_1 = self.conv3_1(y)\n        y_2 = self.conv3_2(y)\n        y = concatenate([y_1, y_2], axis=3)\n        \n        z = self.conv1_3(inputs)\n        z = self.conv3_3(z)\n        z = self.conv3_4(z)\n        z_1 = self.conv3_5(z)\n        z_2 = self.conv3_6(z)\n        z = concatenate([z_1, z_2], axis=3)\n        \n        w = self.conv1_4(inputs)\n        w = self.conv3_7(w)\n        w = self.mp(w)\n        \n        outputs = concatenate([x, y, z, w], axis=3)\n        \n        return outputs","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nSpatial Transformer Network    \n    \nIntroduces in [1], these networks learn an affine transformation to get the interesting part of the image into focus\nThis implementation is borrowed from [2], and a lot was learnt prior to that from [3]\n    \nReferences\n    ----------\n    [1]  Spatial Transformer Networks, Max Jaderberg, et al., 2015\n    [2]  https://github.com/mbanf/STN.keras\n    [3]  https://github.com/hello2all/GTSRB_Keras_STN\n\"\"\"\n\ndef K_meshgrid(x, y):\n    return tf.meshgrid(x, y)\n\ndef K_linspace(start, stop, num):\n    return tf.linspace(start, stop, num)\n\nclass BilinearInterpolation(Layer):\n\n    def __init__(self, output_size, **kwargs):\n        self.output_size = output_size\n        super(BilinearInterpolation, self).__init__(**kwargs)\n\n#     def compute_output_shape(self, input_shapes):\n#         height, width, num_channels = self.output_size\n#         #num_channels = input_shapes[0][-1]\n#         return (None, height, width, num_channels)\n\n    def call(self, tensors, mask=None):\n        X, transformation = tensors\n        output = self._transform(X, transformation, self.output_size)\n        return output\n\n    def _interpolate(self, image, sampled_grids, output_size):\n\n        batch_size = K.shape(image)[0]\n        height = K.shape(image)[1]\n        width = K.shape(image)[2]\n        num_channels = K.shape(image)[3]\n\n        x = K.cast(K.flatten(sampled_grids[:, 0:1, :]), dtype='float32')\n        y = K.cast(K.flatten(sampled_grids[:, 1:2, :]), dtype='float32')\n\n        x = .5 * (x + 1.0) * K.cast(width, dtype='float32')\n        y = .5 * (y + 1.0) * K.cast(height, dtype='float32')\n\n        x0 = K.cast(x, 'int32')\n        x1 = x0 + 1\n        y0 = K.cast(y, 'int32')\n        y1 = y0 + 1\n\n        max_x = int(K.int_shape(image)[2] - 1)\n        max_y = int(K.int_shape(image)[1] - 1)\n\n        x0 = K.clip(x0, 0, max_x)\n        x1 = K.clip(x1, 0, max_x)\n        y0 = K.clip(y0, 0, max_y)\n        y1 = K.clip(y1, 0, max_y)\n\n        pixels_batch = K.arange(0, batch_size) * (height * width)\n        pixels_batch = K.expand_dims(pixels_batch, axis=-1)\n        flat_output_size = output_size[0] * output_size[1]\n        base = K.repeat_elements(pixels_batch, flat_output_size, axis=1)\n        base = K.flatten(base)\n\n        base_y0 = y0 * width\n        base_y0 = base + base_y0\n        base_y1 = y1 * width\n        base_y1 = base_y1 + base\n\n        indices_a = base_y0 + x0\n        indices_b = base_y1 + x0\n        indices_c = base_y0 + x1\n        indices_d = base_y1 + x1\n\n        flat_image = K.reshape(image, shape=(-1, num_channels))\n        flat_image = K.cast(flat_image, dtype='float32')\n        pixel_values_a = K.gather(flat_image, indices_a)\n        pixel_values_b = K.gather(flat_image, indices_b)\n        pixel_values_c = K.gather(flat_image, indices_c)\n        pixel_values_d = K.gather(flat_image, indices_d)\n\n        x0 = K.cast(x0, 'float32')\n        x1 = K.cast(x1, 'float32')\n        y0 = K.cast(y0, 'float32')\n        y1 = K.cast(y1, 'float32')\n\n        area_a = K.expand_dims(((x1 - x) * (y1 - y)), 1)\n        area_b = K.expand_dims(((x1 - x) * (y - y0)), 1)\n        area_c = K.expand_dims(((x - x0) * (y1 - y)), 1)\n        area_d = K.expand_dims(((x - x0) * (y - y0)), 1)\n\n        values_a = area_a * pixel_values_a\n        values_b = area_b * pixel_values_b\n        values_c = area_c * pixel_values_c\n        values_d = area_d * pixel_values_d\n        return values_a + values_b + values_c + values_d\n\n    def _make_regular_grids(self, batch_size, height, width):\n        # making a single regular grid\n        x_linspace = K_linspace(-1., 1., width)\n        y_linspace = K_linspace(-1., 1., height)\n        x_coordinates, y_coordinates = K_meshgrid(x_linspace, y_linspace)\n        x_coordinates = K.flatten(x_coordinates)\n        y_coordinates = K.flatten(y_coordinates)\n        ones = K.ones_like(x_coordinates)\n        grid = K.concatenate([x_coordinates, y_coordinates, ones], 0)\n\n        # repeating grids for each batch\n        grid = K.flatten(grid)\n        grids = K.tile(grid, K.stack([batch_size]))\n        return K.reshape(grids, (batch_size, 3, height * width))\n\n    def _transform(self, X, affine_transformation, output_size):\n        batch_size = K.shape(X)[0]\n        transformations = K.reshape(affine_transformation, shape=(batch_size, 2, 3))\n        regular_grids = self._make_regular_grids(batch_size, output_size[0], output_size[1])\n        sampled_grids = K.batch_dot(transformations, regular_grids)\n        interpolated_image = self._interpolate(X, sampled_grids, output_size)\n        new_shape = (batch_size, output_size[0], output_size[1], output_size[2])\n        interpolated_image = K.reshape(interpolated_image, new_shape)\n        return interpolated_image\n    \ndef get_initial_weights(output_size):\n    b = np.zeros((2, 3), dtype='float32')\n    b[0, 0] = 1\n    b[1, 1] = 1\n    W = np.zeros((output_size, 6), dtype='float32')\n    weights = [W, b.flatten()]\n    return weights\n\n\ndef STN(input_shape=(32, 32, 3), filter_size=5):\n    image = Input(shape=input_shape)\n    locnet = Conv2D(64, 5, strides=2, activation='relu')(image)\n    locnet = MaxPool2D()(locnet)\n    locnet = Conv2D(128, filter_size, activation='relu')(locnet)\n    locnet = Flatten()(locnet)\n    locnet = Dense(128)(locnet)\n    weights = get_initial_weights(128)\n    locnet = Dense(6, weights=weights)(locnet)\n    out = BilinearInterpolation(input_shape)([image, locnet])\n    return Model(inputs=image, outputs=out)\n\n\"\"\"\nHere's an example usage of STN()\n\nm = Sequential()\nm.add(STN(input_shape=(16,16,3), filter_size=3))\nm.add(Conv2D(64, 3, padding='same'))\nm.add(MaxPool2D())\nm.add(Conv2D(64, 3, padding='same'))\nm.add(Dropout(0.5))\nm.add(Flatten())\nm.add(Dense(43, activation='softmax'))\n\nm.summary()\nm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\"\"\"","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"\"\\nHere's an example usage of STN()\\n\\nm = Sequential()\\nm.add(STN(input_shape=(16,16,3), filter_size=3))\\nm.add(Conv2D(64, 3, padding='same'))\\nm.add(MaxPool2D())\\nm.add(Conv2D(64, 3, padding='same'))\\nm.add(Dropout(0.5))\\nm.add(Flatten())\\nm.add(Dense(43, activation='softmax'))\\n\\nm.summary()\\nm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\\n\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nA Deep Convolutional Neural Network\n\nModel inspired by [1], but uses more recent Inception Modules  \n\nReferences\n    [1] Traffic Sign Classification Using Deep Inception Based Convolutional Networks, Mrinal Hanoi, 2015\n\n\"\"\"\n\ninput_shape = (32,32,3)\n\nmodel = Sequential()\nmodel.add(STN(input_shape))\nmodel.add(Conv2D(64, 3, activation=PReLU(), padding='same'))\nmodel.add(STN((32,32,64)))\nmodel.add(Conv2D(64, 3, activation=PReLU(), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D())\n\nmodel.add(STN((16,16,64), 3))\nmodel.add(Inception4A(32, 64, 16, 16))\nmodel.add(STN((16,16,128), 3))\nmodel.add(Inception4A(32, 64, 16, 16))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D())\n\nmodel.add(Inception4B(64, 128, 32, 32))\nmodel.add(Inception4B(64, 128, 32, 32))\nmodel.add(Inception4B(64, 128, 32, 32))\nmodel.add(Inception4B(64, 128, 32, 32))\nmodel.add(Inception4B(64, 128, 32, 32))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D())\n\nmodel.add(Inception4C(128, 256, 64, 64))\nmodel.add(Inception4C(128, 256, 64, 64))\nmodel.add(AvgPool2D(pool_size=4))\n\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(43, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()","execution_count":11,"outputs":[{"output_type":"stream","text":"Model: \"sequential_6\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nmodel_22 (Model)             (None, 32, 32, 3)         358150    \n_________________________________________________________________\nconv2d_481 (Conv2D)          (None, 32, 32, 64)        67328     \n_________________________________________________________________\nmodel_23 (Model)             (None, 32, 32, 64)        455750    \n_________________________________________________________________\nconv2d_484 (Conv2D)          (None, 32, 32, 64)        102464    \n_________________________________________________________________\nbatch_normalization_14 (Batc (None, 32, 32, 64)        256       \n_________________________________________________________________\nmax_pooling2d_83 (MaxPooling (None, 16, 16, 64)        0         \n_________________________________________________________________\nmodel_24 (Model)             (None, 16, 16, 64)        193606    \n_________________________________________________________________\ninception4a_8 (Inception4A)  (None, 16, 16, 128)       67056     \n_________________________________________________________________\nmodel_25 (Model)             (None, 16, 16, 128)       296006    \n_________________________________________________________________\ninception4a_9 (Inception4A)  (None, 16, 16, 128)       75248     \n_________________________________________________________________\nbatch_normalization_15 (Batc (None, 16, 16, 128)       512       \n_________________________________________________________________\nmax_pooling2d_88 (MaxPooling (None, 8, 8, 128)         0         \n_________________________________________________________________\ninception4b_17 (Inception4B) (None, 8, 8, 256)         286368    \n_________________________________________________________________\ninception4b_18 (Inception4B) (None, 8, 8, 256)         319136    \n_________________________________________________________________\ninception4b_19 (Inception4B) (None, 8, 8, 256)         319136    \n_________________________________________________________________\ninception4b_20 (Inception4B) (None, 8, 8, 256)         319136    \n_________________________________________________________________\ninception4b_21 (Inception4B) (None, 8, 8, 256)         319136    \n_________________________________________________________________\nbatch_normalization_16 (Batc (None, 8, 8, 256)         1024      \n_________________________________________________________________\nmax_pooling2d_94 (MaxPooling (None, 4, 4, 256)         0         \n_________________________________________________________________\ninception4c_8 (Inception4C)  (None, 4, 4, 512)         361472    \n_________________________________________________________________\ninception4c_9 (Inception4C)  (None, 4, 4, 512)         492544    \n_________________________________________________________________\naverage_pooling2d_4 (Average (None, 1, 1, 512)         0         \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 1, 1, 512)         0         \n_________________________________________________________________\nflatten_30 (Flatten)         (None, 512)               0         \n_________________________________________________________________\ndense_56 (Dense)             (None, 43)                22059     \n=================================================================\nTotal params: 4,056,387\nTrainable params: 4,055,491\nNon-trainable params: 896\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs))\nepochs = 20\n\nh = model.fit(data['x_train'], data['y_train'],\n              batch_size=20, epochs=epochs,\n              validation_data=(data['x_validation'], data['y_validation']),\n              callbacks=[annealer], verbose=1)\n\n# Test Accuracy\npred = model.predict(data['x_test'])\ncorr = np.argmax(pred, axis=1)\nacc = np.mean(corr == data['y_test'])\nprint(\"Test accuracy: \", acc)","execution_count":23,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n8699/8699 [==============================] - 437s 50ms/step - loss: 4.0745 - accuracy: 0.0249 - val_loss: 3.7620 - val_accuracy: 0.0136 - lr: 5.9874e-04\nEpoch 2/10\n8699/8699 [==============================] - 434s 50ms/step - loss: 3.7623 - accuracy: 0.0238 - val_loss: 3.7703 - val_accuracy: 0.0136 - lr: 5.6880e-04\nEpoch 3/10\n8699/8699 [==============================] - 437s 50ms/step - loss: 3.7624 - accuracy: 0.0225 - val_loss: 3.7634 - val_accuracy: 0.0136 - lr: 5.4036e-04\nEpoch 4/10\n8699/8699 [==============================] - 470s 54ms/step - loss: 3.7621 - accuracy: 0.0232 - val_loss: 3.7646 - val_accuracy: 0.0272 - lr: 5.1334e-04\nEpoch 5/10\n8699/8699 [==============================] - 480s 55ms/step - loss: 3.7621 - accuracy: 0.0232 - val_loss: 3.7694 - val_accuracy: 0.0204 - lr: 4.8767e-04\nEpoch 6/10\n4805/8699 [===============>..............] - ETA: 3:32 - loss: 3.7623 - accuracy: 0.0219","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-9484a1730eb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m               callbacks=[annealer], verbose=1)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Test Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}
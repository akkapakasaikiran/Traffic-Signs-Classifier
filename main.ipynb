{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pickle \nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom math import sqrt, ceil\nfrom keras.utils.np_utils import to_categorical\n\nimport tensorflow as tf\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dropout, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, PReLU, Flatten, Dense, Input\nfrom keras.layers import Layer, concatenate\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.models import Model\nfrom keras.utils import plot_model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n\"\"\"\nDataset used is a preprocessed version of the German Traffic Signs Recognition Benchmark (GTSRB) dataset, details about which\ncan be found at [2]. This dataset is picked from [1]. The following code for reading data is borrowed from [3].\nIn particular data2.pickle is used, which performs shuffling and incorporates division by 255.0 followed by Mean Normalisation.\n \n\nReferences\n    [1] https://www.kaggle.com/valentynsichkar/traffic-signs-preprocessed\n    [2] https://www.kaggle.com/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign\n    [3] https://www.kaggle.com/valentynsichkar/traffic-signs-classification-with-cnn\n\"\"\"\n\n# Opening file for reading in binary mode\nwith open('../input/traffic-signs-preprocessed/data2.pickle', 'rb') as f:\n    data = pickle.load(f, encoding='latin1')  # dictionary type\n\n# Preparing y_train and y_validation for using in Keras\ndata['y_train'] = to_categorical(data['y_train'], num_classes=43)\ndata['y_validation'] = to_categorical(data['y_validation'], num_classes=43)\n\n# Making channels come at the end\ndata['x_train'] = data['x_train'].transpose(0, 2, 3, 1)\ndata['x_validation'] = data['x_validation'].transpose(0, 2, 3, 1)\ndata['x_test'] = data['x_test'].transpose(0, 2, 3, 1)\n\n# Showing loaded data from file\nfor i, j in data.items():\n    if i == 'labels':\n        print(i + ':', len(j))\n    else: \n        print(i + ':', j.shape)","execution_count":7,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-7-4c2714b3f4f1>, line 26)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-4c2714b3f4f1>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    Showing loaded data from file\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"          \n\"\"\"\nInception Modules\n\nAn implementation of Google's Inception modules, introduced in GoogLeNet ([1]), done from scrath using Layer Subclassing.\nThe design of these implementations differs a bit from that of the original, and the deviations are inspired by [2].\n\nReferences\n    [1] Going deeper with convolutions, Christian Szegedy, Wei Liu, 2015\n    [2] Traffic Sign Classification Using Deep Inception Based Convolutional Networks, Mrinal Hanoi, 2015\n\n\"\"\"\nclass InceptionA(Layer):\n    def __init__(self, ch_out_1, ch_out_3, ch_out_5, ch_out_m):\n        super(InceptionA, self).__init__()\n        self.conv1_1 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        \n        self.conv1_2 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_1 = Conv2D(ch_out_3, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_3 = Conv2D(ch_out_1, 1, activation=\"relu\", padding=\"same\")\n        self.conv5_1 = Conv2D(ch_out_5, 5, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_4 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_2 = Conv2D(ch_out_m, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.mp = MaxPool2D(pool_size=3, padding=\"same\", strides=1)\n        \n        \n    def call(self, inputs):\n        x = self.conv1_1(inputs)\n        \n        y = self.conv1_2(inputs)\n        y = self.conv3_1(y)\n        \n        z = self.conv1_3(inputs)\n        z = self.conv5_1(z)\n        \n        w = self.conv1_4(inputs)\n        w = self.conv3_2(w)\n        w = self.mp(w)\n        \n        outputs = concatenate([x,y,z,w], axis=3)\n        \n        return outputs\n    \n    \nclass Inception4A(Layer):\n    def __init__(self, ch_out_1, ch_out_3, ch_out_5, ch_out_m):\n        super(Inception4A, self).__init__()\n        self.conv1_1 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        \n        self.conv1_2 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_1 = Conv2D(ch_out_3, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_3 = Conv2D(ch_out_1, 1, activation=\"relu\", padding=\"same\")\n        self.conv3_2 = Conv2D(ch_out_5, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv3_3 = Conv2D(ch_out_5, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_4 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_4 = Conv2D(ch_out_m, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.mp = MaxPool2D(pool_size=3, padding=\"same\", strides=1)\n        \n        \n    def call(self, inputs):\n        x = self.conv1_1(inputs)\n        \n        y = self.conv1_2(inputs)\n        y = self.conv3_1(y)\n        \n        z = self.conv1_3(inputs)\n        z = self.conv3_2(z)\n        z = self.conv3_3(z)\n        \n        w = self.conv1_4(inputs)\n        w = self.conv3_4(w)\n        w = self.mp(w)\n        \n        outputs = concatenate([x,y,z,w], axis=3)\n        \n        return outputs\n    \nclass Inception4B(Layer):\n    def __init__(self, ch_out_1, ch_out_3, ch_out_5, ch_out_m):\n        super(Inception4B, self).__init__()\n        self.conv1_1 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        \n        self.conv1_2 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv7_1 = Conv2D(ch_out_3, (7,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv7_2 = Conv2D(ch_out_3, (1,7), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_3 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv7_3 = Conv2D(ch_out_5, (7,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv7_4 = Conv2D(ch_out_5, (1,7), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv7_5 = Conv2D(ch_out_5, (7,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv7_6 = Conv2D(ch_out_5, (1,7), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_4 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_1 = Conv2D(ch_out_m, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.mp = MaxPool2D(pool_size=3, padding=\"same\", strides=1)\n        \n        \n    def call(self, inputs):\n        x = self.conv1_1(inputs)\n        \n        y = self.conv1_2(inputs)\n        y = self.conv7_1(y)\n        y = self.conv7_2(y)\n        \n        z = self.conv1_3(inputs)\n        z = self.conv7_3(z)\n        z = self.conv7_4(z)\n        z = self.conv7_5(z)\n        z = self.conv7_6(z)\n        \n        w = self.conv1_4(inputs)\n        w = self.conv3_1(w)\n        w = self.mp(w)\n        \n        outputs = concatenate([x,y,z,w], axis=3)\n        \n        return outputs\n    \nclass Inception4C(Layer):\n    def __init__(self, ch_out_1, ch_out_3, ch_out_5, ch_out_m):\n        super(Inception4C, self).__init__()\n        self.conv1_1 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        \n        self.conv1_2 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_1 = Conv2D(ch_out_3/2, (1,3), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv3_2 = Conv2D(ch_out_3/2, (3,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_3 = Conv2D(ch_out_1, 1, activation=\"relu\", padding=\"same\")\n        self.conv3_3 = Conv2D(ch_out_5, (1,3), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv3_4 = Conv2D(ch_out_5, (3,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv3_5 = Conv2D(ch_out_5/2, (1,3), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.conv3_6 = Conv2D(ch_out_5/2, (3,1), padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        \n        self.conv1_4 = Conv2D(ch_out_1, 1, activation=\"relu\")\n        self.conv3_7 = Conv2D(ch_out_m, 3, padding=\"same\", activation=PReLU(), kernel_regularizer='l2')\n        self.mp = MaxPool2D(pool_size=3, padding=\"same\", strides=1)\n        \n        \n    def call(self, inputs):\n        x = self.conv1_1(inputs)\n        \n        y = self.conv1_2(inputs)\n        y_1 = self.conv3_1(y)\n        y_2 = self.conv3_2(y)\n        y = concatenate([y_1, y_2], axis=3)\n        \n        z = self.conv1_3(inputs)\n        z = self.conv3_3(z)\n        z = self.conv3_4(z)\n        z_1 = self.conv3_5(z)\n        z_2 = self.conv3_6(z)\n        z = concatenate([z_1, z_2], axis=3)\n        \n        w = self.conv1_4(inputs)\n        w = self.conv3_7(w)\n        w = self.mp(w)\n        \n        outputs = concatenate([x, y, z, w], axis=3)\n        \n        return outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nSpatial Transformer Network    \n    \nIntroduced in [1], these networks learn an affine transformation to get the interesting part of the image into focus.\nThis implementation is borrowed from [2], and a lot was learnt prior to that from [3].\n    \nHere's an example usage of STN()\n\nm = Sequential()\nm.add(STN(input_shape=(16,16,3), filter_size=3))\nm.add(Conv2D(64, 3, padding='same'))\nm.add(MaxPool2D())\nm.add(Conv2D(64, 3, padding='same'))\nm.add(Dropout(0.5))\nm.add(Flatten())\nm.add(Dense(43, activation='softmax'))\n\nm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nm.summary()\n\nReferences\n    ----------\n    [1]  Spatial Transformer Networks, Max Jaderberg, et al., 2015\n    [2]  https://github.com/mbanf/STN.keras\n    [3]  https://github.com/hello2all/GTSRB_Keras_STN\n\"\"\"\n\ndef K_meshgrid(x, y):\n    return tf.meshgrid(x, y)\n\ndef K_linspace(start, stop, num):\n    return tf.linspace(start, stop, num)\n\nclass BilinearInterpolation(Layer):\n\n    def __init__(self, output_size, **kwargs):\n        self.output_size = output_size\n        super(BilinearInterpolation, self).__init__(**kwargs)\n\n    def call(self, tensors, mask=None):\n        X, transformation = tensors\n        output = self._transform(X, transformation, self.output_size)\n        return output\n\n    def _interpolate(self, image, sampled_grids, output_size):\n\n        batch_size = K.shape(image)[0]\n        height = K.shape(image)[1]\n        width = K.shape(image)[2]\n        num_channels = K.shape(image)[3]\n\n        x = K.cast(K.flatten(sampled_grids[:, 0:1, :]), dtype='float32')\n        y = K.cast(K.flatten(sampled_grids[:, 1:2, :]), dtype='float32')\n\n        x = .5 * (x + 1.0) * K.cast(width, dtype='float32')\n        y = .5 * (y + 1.0) * K.cast(height, dtype='float32')\n\n        x0 = K.cast(x, 'int32')\n        x1 = x0 + 1\n        y0 = K.cast(y, 'int32')\n        y1 = y0 + 1\n\n        max_x = int(K.int_shape(image)[2] - 1)\n        max_y = int(K.int_shape(image)[1] - 1)\n\n        x0 = K.clip(x0, 0, max_x)\n        x1 = K.clip(x1, 0, max_x)\n        y0 = K.clip(y0, 0, max_y)\n        y1 = K.clip(y1, 0, max_y)\n\n        pixels_batch = K.arange(0, batch_size) * (height * width)\n        pixels_batch = K.expand_dims(pixels_batch, axis=-1)\n        flat_output_size = output_size[0] * output_size[1]\n        base = K.repeat_elements(pixels_batch, flat_output_size, axis=1)\n        base = K.flatten(base)\n\n        base_y0 = y0 * width\n        base_y0 = base + base_y0\n        base_y1 = y1 * width\n        base_y1 = base_y1 + base\n\n        indices_a = base_y0 + x0\n        indices_b = base_y1 + x0\n        indices_c = base_y0 + x1\n        indices_d = base_y1 + x1\n\n        flat_image = K.reshape(image, shape=(-1, num_channels))\n        flat_image = K.cast(flat_image, dtype='float32')\n        pixel_values_a = K.gather(flat_image, indices_a)\n        pixel_values_b = K.gather(flat_image, indices_b)\n        pixel_values_c = K.gather(flat_image, indices_c)\n        pixel_values_d = K.gather(flat_image, indices_d)\n\n        x0 = K.cast(x0, 'float32')\n        x1 = K.cast(x1, 'float32')\n        y0 = K.cast(y0, 'float32')\n        y1 = K.cast(y1, 'float32')\n\n        area_a = K.expand_dims(((x1 - x) * (y1 - y)), 1)\n        area_b = K.expand_dims(((x1 - x) * (y - y0)), 1)\n        area_c = K.expand_dims(((x - x0) * (y1 - y)), 1)\n        area_d = K.expand_dims(((x - x0) * (y - y0)), 1)\n\n        values_a = area_a * pixel_values_a\n        values_b = area_b * pixel_values_b\n        values_c = area_c * pixel_values_c\n        values_d = area_d * pixel_values_d\n        return values_a + values_b + values_c + values_d\n\n    def _make_regular_grids(self, batch_size, height, width):\n        # making a single regular grid\n        x_linspace = K_linspace(-1., 1., width)\n        y_linspace = K_linspace(-1., 1., height)\n        x_coordinates, y_coordinates = K_meshgrid(x_linspace, y_linspace)\n        x_coordinates = K.flatten(x_coordinates)\n        y_coordinates = K.flatten(y_coordinates)\n        ones = K.ones_like(x_coordinates)\n        grid = K.concatenate([x_coordinates, y_coordinates, ones], 0)\n\n        # repeating grids for each batch\n        grid = K.flatten(grid)\n        grids = K.tile(grid, K.stack([batch_size]))\n        return K.reshape(grids, (batch_size, 3, height * width))\n\n    def _transform(self, X, affine_transformation, output_size):\n        batch_size = K.shape(X)[0]\n        transformations = K.reshape(affine_transformation, shape=(batch_size, 2, 3))\n        regular_grids = self._make_regular_grids(batch_size, output_size[0], output_size[1])\n        sampled_grids = K.batch_dot(transformations, regular_grids)\n        interpolated_image = self._interpolate(X, sampled_grids, output_size)\n        new_shape = (batch_size, output_size[0], output_size[1], output_size[2])\n        interpolated_image = K.reshape(interpolated_image, new_shape)\n        return interpolated_image\n    \ndef get_initial_weights(output_size):\n    b = np.zeros((2, 3), dtype='float32')\n    b[0, 0] = 1\n    b[1, 1] = 1\n    W = np.zeros((output_size, 6), dtype='float32')\n    weights = [W, b.flatten()]\n    return weights\n\n\ndef STN(input_shape=(32, 32, 3), filter_size=5):\n    image = Input(shape=input_shape)\n    locnet = Conv2D(64, 5, strides=2, activation='relu')(image)\n    locnet = MaxPool2D()(locnet)\n    locnet = Conv2D(128, filter_size, activation='relu')(locnet)\n    locnet = Flatten()(locnet)\n    locnet = Dense(128)(locnet)\n    weights = get_initial_weights(128)\n    locnet = Dense(6, weights=weights)(locnet)\n    out = BilinearInterpolation(input_shape)([image, locnet])\n    return Model(inputs=image, outputs=out)\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nA Deep Convolutional Neural Network\n\nModel inspired by [1], but uses more recent Inception Modules. \nThe presented network consists of spatial transformer layers and modified versions of inception modules specifically designed for capturing \nlocal and global features together. This features adoption allows the network to classify precisely intraclass samples even under deformations.\nUse of spatial transformer layer makes this network more robust to deformations such as translation, rotation, scaling of input images.\n\nReferences\n    ---------\n    [1] Traffic Sign Classification Using Deep Inception Based Convolutional Networks, Mrinal Hanoi, 2015\n\n\"\"\"\n\ninput_shape = (32,32,3)\n\nmodel = Sequential()\nmodel.add(STN(input_shape))\nmodel.add(Conv2D(64, 3, activation=PReLU(), padding='same'))\nmodel.add(STN((32,32,64)))\nmodel.add(Conv2D(64, 3, activation=PReLU(), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D())\n\nmodel.add(STN((16,16,64), 3))\nmodel.add(Inception4A(32, 64, 16, 16))\nmodel.add(STN((16,16,128), 3))\nmodel.add(Inception4A(32, 64, 16, 16))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D())\n\nmodel.add(Inception4B(64, 128, 32, 32))\nmodel.add(Inception4B(64, 128, 32, 32))\nmodel.add(Inception4B(64, 128, 32, 32))\nmodel.add(Inception4B(64, 128, 32, 32))\nmodel.add(Inception4B(64, 128, 32, 32))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D())\n\nmodel.add(Inception4C(128, 256, 64, 64))\nmodel.add(Inception4C(128, 256, 64, 64))\nmodel.add(AvgPool2D(pool_size=4))\n\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(43, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()","execution_count":9,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nmodel (Model)                (None, 32, 32, 3)         358150    \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 32, 32, 64)        67328     \n_________________________________________________________________\nmodel_1 (Model)              (None, 32, 32, 64)        455750    \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 32, 32, 64)        102464    \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 32, 32, 64)        256       \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n_________________________________________________________________\nmodel_2 (Model)              (None, 16, 16, 64)        193606    \n_________________________________________________________________\ninception4a (Inception4A)    (None, 16, 16, 128)       67056     \n_________________________________________________________________\nmodel_3 (Model)              (None, 16, 16, 128)       296006    \n_________________________________________________________________\ninception4a_1 (Inception4A)  (None, 16, 16, 128)       75248     \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 16, 16, 128)       512       \n_________________________________________________________________\nmax_pooling2d_8 (MaxPooling2 (None, 8, 8, 128)         0         \n_________________________________________________________________\ninception4b (Inception4B)    (None, 8, 8, 256)         286368    \n_________________________________________________________________\ninception4b_1 (Inception4B)  (None, 8, 8, 256)         319136    \n_________________________________________________________________\ninception4b_2 (Inception4B)  (None, 8, 8, 256)         319136    \n_________________________________________________________________\ninception4b_3 (Inception4B)  (None, 8, 8, 256)         319136    \n_________________________________________________________________\ninception4b_4 (Inception4B)  (None, 8, 8, 256)         319136    \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 8, 8, 256)         1024      \n_________________________________________________________________\nmax_pooling2d_14 (MaxPooling (None, 4, 4, 256)         0         \n_________________________________________________________________\ninception4c (Inception4C)    (None, 4, 4, 512)         361472    \n_________________________________________________________________\ninception4c_1 (Inception4C)  (None, 4, 4, 512)         492544    \n_________________________________________________________________\naverage_pooling2d (AveragePo (None, 1, 1, 512)         0         \n_________________________________________________________________\ndropout (Dropout)            (None, 1, 1, 512)         0         \n_________________________________________________________________\nflatten_5 (Flatten)          (None, 512)               0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 43)                22059     \n=================================================================\nTotal params: 4,056,387\nTrainable params: 4,055,491\nNon-trainable params: 896\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nTraining and Testing\n\nAn annealer is used to control lerning rate, reducing it as epochs increase so that fine learning can take place. \nIdea attributed to [1].\n\nReferences \n    -----------\n    [1] https://www.kaggle.com/valentynsichkar/traffic-signs-classification-with-cnn\n\"\"\"\n\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs))\nepochs = 30\n\nh = model.fit(data['x_train'], data['y_train'],\n              batch_size=20, epochs=epochs,\n              validation_data=(data['x_validation'], data['y_validation']),\n              callbacks=[annealer], verbose=1)\n\n# Test Accuracy\npred = model.predict(data['x_test'])\ncorr = np.argmax(pred, axis=1)\nacc = np.mean(corr == data['y_test'])\nprint(\"Test accuracy: \", acc)","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch 1/30\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}